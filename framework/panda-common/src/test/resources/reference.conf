panda = {
  id = null
  input-topic = {
    broker = "localhost:9092"
    lock = {
      master = "localhost:2181"
    }
    message = {
      topic = "PandaInput"
      key-class = "java.lang.String"
      message-class = "java.lang.String"
      key-decoder-class = "org.apache.kafka.common.serialization.StringDeserializer"
      message-decoder-class = "org.apache.kafka.common.serialization.StringDeserializer"
    }
  }
  update-topic = {
    broker = "localhost:9092"
    lock = {
      master = "localhost:2181"
    }
    message = {
      topic = "PandaUpdate"
      decoder-class = "org.apache.kafka.common.serialization.StringDeserializer"
      encoder-class = "org.apache.kafka.common.serialization.StringDeserializer"
      max-size = 16777216
    }
  }

  default-streaming-config = {
    spark.io.compression.codec = "lzf"
    spark.speculation = true
    spark.logConf = true
    spark.serializer = "org.apache.spark.serializer.KryoSerializer"
    spark.ui.showConsoleProgress = false
  }

  batch = {
    streaming = {
      master = "yarn"
      deploy-mode = "client"
      generation-interval-sec = 21600
      num-executors = 4
      executor-cores = 4
      executor-memory = "2g"
      driver-memory = "1g"
      dynamic-allocation = false
      config = ${panda.default-streaming-config}
    }
    update-class = null
    storage = {
      data-dir = "file:/tmp/Panda/data/"
      model-dir = "file:/tmp/Panda/model/"
      key-writable-class = "org.apache.hadoop.io.Text"
      message-writable-class = "org.apache.hadoop.io.Text"
      max-age-data-hours = -1
      max-age-model-hours = -1
    }
    ui = {
      port = 4040
    }
  }

  speed = {
    streaming = {
      master = "yarn"
      deploy-mode = "client"
      generation-interval-sec = 10
      num-executors = 2
      executor-cores = 4
      executor-memory = "1g"
      driver-memory = "512m"
      dynamic-allocation = false
      config = ${panda.default-streaming-config}
    }
    model-manager-class = null
    min-model-load-fraction = 0.8
    ui = {
      port = 4040
    }
  }

  serving = {
    memory = "4000m"
    yarn = {
      instances = 1
      cores = "4"
    }
    api = {
      port = 80
      secure-port = 443
      user-name = null
      password = null
      keystore-file = null
      keystore-password = null
      key-alias = null
      read-only = false
      context-path = "/"
    }
    application-resources = null
    model-manager-class = null
    min-model-load-fraction = 0.8
    no-init-topics = false
  }
  ml = {
    eval = {
      test-fraction = 0.1
      candidates = 1
      hyperparam-search = "random"
      parallelism = 1
      threshold = null
    }
  }
}
